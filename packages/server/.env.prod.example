# Production Environment Configuration Example
# This file shows recommended production configuration.
# DO NOT use .env files in production - set environment variables directly!
#
# Use this as a reference for what environment variables to set in:
# - Docker Compose environment section
# - Kubernetes secrets/configmaps
# - Cloud platform environment configuration
# - CI/CD deployment pipelines

# Flask API Configuration (Production)
FLASK_HOST=0.0.0.0
FLASK_PORT=5000
FLASK_DEBUG=False  # CRITICAL: Always False in production

# Database Configuration (Production)
# Use managed PostgreSQL service (AWS RDS, GCP Cloud SQL, Azure Database, etc.)
# DO NOT commit actual connection strings - use secret management
DATABASE_URL=postgresql://user:password@db-host:5432/insighthub_prod

# Service Implementation Configuration (Production)

# Repository Implementation Configuration (Production)
USER_REPOSITORY_TYPE=sql
DOCUMENT_REPOSITORY_TYPE=sql
CHAT_SESSION_REPOSITORY_TYPE=sql
CHAT_MESSAGE_REPOSITORY_TYPE=sql

# Blob Storage Implementation Configuration (Production)
# Use S3 for scalability and reliability
BLOB_STORAGE_TYPE=s3

# File System Storage Configuration (Production)
# Not used when BLOB_STORAGE_TYPE=s3, but kept as fallback
FILE_SYSTEM_STORAGE_PATH=/var/lib/insighthub/uploads

# S3 Storage Configuration (Production)
# Use AWS S3, DigitalOcean Spaces, Cloudflare R2, or other S3-compatible storage
# DO NOT commit actual credentials - use secret management
S3_ENDPOINT_URL=https://s3.amazonaws.com  # or https://s3.us-east-1.amazonaws.com
S3_ACCESS_KEY=AKIAIOSFODNN7EXAMPLE  # Replace with actual key from secrets manager
S3_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY  # Replace with actual secret
S3_BUCKET_NAME=insighthub-prod-documents

# Upload Configuration (Production)
UPLOAD_FOLDER=/var/lib/insighthub/uploads
MAX_CONTENT_LENGTH=104857600  # 100 MB

# RAG Configuration (Production)
RAG_TYPE=vector
CHUNKING_STRATEGY=sentence
CHUNK_SIZE=512
CHUNK_OVERLAP=50
RAG_TOP_K=5

# OpenAI Configuration (Production)
# Use OpenAI for production-grade LLM and embeddings
# DO NOT commit actual API keys - use secret management
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx  # Replace with actual key from secrets manager

# Ollama Configuration (Production)
# Alternative to OpenAI if running Ollama in production
# Comment out if using OpenAI
# OLLAMA_BASE_URL=http://ollama-service:11434
# OLLAMA_LLM_MODEL=llama3.2
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Qdrant Configuration (Production)
# Use managed Qdrant Cloud or self-hosted Qdrant cluster
QDRANT_HOST=qdrant-service  # Kubernetes service name or cloud endpoint
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=insighthub_prod

# Security Best Practices:
# 1. Never commit this file with real credentials
# 2. Use environment variables or secret management systems:
#    - AWS Secrets Manager / Parameter Store
#    - GCP Secret Manager
#    - Azure Key Vault
#    - HashiCorp Vault
#    - Kubernetes Secrets
# 3. Rotate credentials regularly
# 4. Use different credentials for each environment
# 5. Limit access with IAM policies
