# Local-Only Development Configuration
# This file demonstrates a completely local setup with no external dependencies.
# Perfect for offline development or when you want everything running locally.
#
# Copy this to .env.local and customize as needed.
# .env.local is gitignored and will not be committed.

# Flask API Configuration (Local)
FLASK_HOST=127.0.0.1
FLASK_PORT=5000
FLASK_DEBUG=True

# Database Configuration (Local)
# Uses local PostgreSQL - make sure it's running
DATABASE_URL=postgresql://insighthub:insighthub_dev@localhost:5432/insighthub

# Service Implementation Configuration

# Repository Implementation Configuration
USER_REPOSITORY_TYPE=sql
DOCUMENT_REPOSITORY_TYPE=sql
CHAT_SESSION_REPOSITORY_TYPE=sql
CHAT_MESSAGE_REPOSITORY_TYPE=sql

# Blob Storage Implementation Configuration
# Use local file system - no cloud dependencies
BLOB_STORAGE_TYPE=file_system

# File System Storage Configuration (Local)
# Store files in local uploads directory
FILE_SYSTEM_STORAGE_PATH=uploads

# Upload Configuration (Local)
UPLOAD_FOLDER=uploads
MAX_CONTENT_LENGTH=104857600  # 100 MB

# RAG Configuration (Local)
RAG_TYPE=vector
CHUNKING_STRATEGY=sentence
CHUNK_SIZE=512
CHUNK_OVERLAP=50
RAG_TOP_K=5

# Ollama Configuration (Local)
# Run Ollama locally: ollama serve
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_LLM_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Qdrant Configuration (Local)
# Run Qdrant locally: docker run -p 6333:6333 qdrant/qdrant
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=insighthub_local

# OpenAI Configuration (Local)
# Leave empty to use Ollama instead
OPENAI_API_KEY=

# NOTE: S3/MinIO configuration is not needed for local-only setup
# Uncomment below if you want to use local MinIO instead of file system:
#
# BLOB_STORAGE_TYPE=s3
# S3_ENDPOINT_URL=http://localhost:9000
# S3_ACCESS_KEY=minioadmin
# S3_SECRET_KEY=minioadmin
# S3_BUCKET_NAME=insighthub-local
