services:
    # PostgreSQL database
    postgres:
        image: postgres:16-alpine
        container_name: insighthub-postgres
        ports:
            - '5432:5432'
        environment:
            - POSTGRES_USER=insighthub
            - POSTGRES_PASSWORD=insighthub_dev
            - POSTGRES_DB=insighthub
        volumes:
            - postgres_data:/var/lib/postgresql/data
        healthcheck:
            test: ['CMD-SHELL', 'pg_isready -U insighthub']
            interval: 10s
            timeout: 5s
            retries: 5
        restart: unless-stopped

    # MinIO object storage (S3-compatible)
    minio:
        image: minio/minio:latest
        container_name: insighthub-minio
        ports:
            - '9000:9000'
            - '9001:9001'
        environment:
            - MINIO_ROOT_USER=insighthub
            - MINIO_ROOT_PASSWORD=insighthub_dev_secret
        volumes:
            - minio_data:/data
        command: server /data --console-address ":9001"
        healthcheck:
            test: ['CMD', 'mc', 'ready', 'local']
            interval: 10s
            timeout: 5s
            retries: 5
        restart: unless-stopped

    # Ollama LLM service
    ollama:
        image: ollama/ollama:latest
        container_name: insighthub-ollama
        ports:
            - '11434:11434'
        volumes:
            - ollama_data:/root/.ollama
        healthcheck:
            test: ['CMD', 'ollama', 'list']
            interval: 10s
            timeout: 5s
            retries: 5
        restart: unless-stopped

    # Ollama setup - pulls required models
    ollama-setup:
        image: ollama/ollama:latest
        container_name: insighthub-ollama-setup
        depends_on:
            ollama:
                condition: service_healthy
        environment:
            - OLLAMA_HOST=ollama:11434
        entrypoint: /bin/sh
        command:
            - -c
            - |
                echo "Waiting for Ollama API to be ready..."
                until ollama list > /dev/null 2>&1; do
                    echo "Waiting for Ollama service..."
                    sleep 2
                done
                echo "Ollama is ready. Pulling llama3.2:1b model..."
                ollama pull llama3.2:1b
                echo "Pulling nomic-embed-text model..."
                ollama pull nomic-embed-text
                echo "Models pulled successfully"
        restart: on-failure

    # Python server - production build
    server:
        build:
            context: ./packages/server
            dockerfile: Dockerfile
            target: production
        container_name: insighthub-server
        ports:
            - '5000:8000'
        volumes:
            - ./packages/server/src:/app/src
        environment:
            - PYTHONUNBUFFERED=1
            - DATABASE_URL=postgresql://insighthub:insighthub_dev@postgres:5432/insighthub
            - BLOB_STORAGE_TYPE=s3
            - S3_ENDPOINT_URL=http://minio:9000
            - S3_ACCESS_KEY=insighthub
            - S3_SECRET_KEY=insighthub_dev_secret
            - S3_BUCKET_NAME=documents
            - LLM_PROVIDER=ollama
            - OLLAMA_BASE_URL=http://ollama:11434
            - OLLAMA_LLM_MODEL=llama3.2:1b
            - OLLAMA_EMBEDDING_MODEL=nomic-embed-text
        depends_on:
            postgres:
                condition: service_healthy
            minio:
                condition: service_healthy
            ollama:
                condition: service_healthy
        restart: unless-stopped

    # React client - production build
    client:
        build:
            context: ./packages/client
            dockerfile: Dockerfile
            target: production
        container_name: insighthub-client
        ports:
            - '3000:80'
        depends_on:
            - server
        restart: unless-stopped

volumes:
    postgres_data:
    minio_data:
    ollama_data:
